# RETFound + LoRA Configuration for Vast.ai (RTX 5090 Optimized)
#
# This configuration is optimized for NVIDIA RTX 5090 (32GB VRAM) on Vast.ai.
# Key optimizations:
# - batch_size: 64 (2x default, utilizes 32GB VRAM efficiently)
# - num_workers: 8 (faster data loading on high-end systems)
# - lora_r: 16 (higher capacity without memory concerns)
# - mixed_precision: true (leverage Tensor Cores for speed)
#
# Expected performance on RTX 5090:
# - Training time: ~3-3.5 hours (20 epochs)
# - Memory usage: ~12GB / 32GB (37% utilization)
# - Potentially better accuracy due to larger batch size

model:
  model_name: retfound_lora
  num_classes: 5
  pretrained: true
  pretrained_path: "/models/RETFound_cfp_weights.pth"  # Volume mount path

  # LoRA-specific parameters (optimized for RTX 5090)
  lora_r: 16                   # Higher rank for better capacity (vs 8 default)
                               # RTX 5090 has plenty of VRAM for r=16

  lora_alpha: 64               # 4x rank (higher scaling for stronger updates)

  lora_dropout: 0.1            # Dropout for LoRA layers

  head_dropout: 0.3            # Dropout before classification head

  target_modules: ["qkv"]      # Apply LoRA to attention QKV projections

training:
  # RTX 5090 optimizations
  batch_size: 64               # 2x larger than default (32)
                               # Utilizes 32GB VRAM efficiently
                               # Leads to more stable gradients

  num_epochs: 20               # Standard training duration

  learning_rate: 0.0005        # 5e-4, works well with larger batches

  weight_decay: 0.01           # L2 regularization

  # Optimizer settings
  optimizer: adamw
  betas: [0.9, 0.999]
  eps: 1.0e-08

  # Learning rate scheduling
  scheduler: cosine            # Cosine annealing
  warmup_epochs: 2             # Warmup for stable training
  min_lr: 1.0e-06              # Minimum LR for cosine annealing

  # Early stopping
  patience: 5
  min_delta: 0.001

  # Gradient clipping
  grad_clip: 1.0               # Prevent exploding gradients

  # Mixed precision training (ESSENTIAL for RTX 5090)
  mixed_precision: true        # Leverage Tensor Cores for 2x speedup

image:
  # RETFound input specifications
  input_size: 224

  # ImageNet normalization (standard for ViT)
  mean: [0.485, 0.456, 0.406]
  std: [0.229, 0.224, 0.225]

  # Data augmentation (applied to training set only)
  augmentation:
    resize: 256                # Resize before random crop
    random_crop: 224           # Random crop to input size
    horizontal_flip: 0.5       # 50% chance of horizontal flip
    vertical_flip: 0.3         # 30% chance of vertical flip
    rotation: 15               # Random rotation up to Â±15 degrees
    color_jitter:
      brightness: 0.2
      contrast: 0.2
      saturation: 0.2
      hue: 0.1
    coarse_dropout:            # Randomly drop patches
      max_holes: 4
      max_height: 20
      max_width: 20
      p: 0.5

data:
  # Vast.ai volume mount paths
  train_csv: "/data/aptos/train.csv"
  test_csv: "/data/aptos/test.csv"
  img_dir: "/data/aptos/train_images"

  # Data split
  val_split: 0.2               # 80/20 train/val split
  stratify: true               # Maintain class distribution

  # Data loading (optimized for RTX 5090)
  num_workers: 8               # 2x default (4), faster data loading
                               # RTX 5090 won't be bottlenecked by data loading

  pin_memory: true             # Faster data transfer to GPU
  persistent_workers: true     # Keep workers alive between epochs

paths:
  # Vast.ai volume mount paths for results
  output_dir: "/results/retfound_lora"
  checkpoint_dir: "/results/retfound_lora/checkpoints"
  log_dir: "/results/retfound_lora/logs"

  # Save checkpoints
  save_every_n_epochs: 5       # Save checkpoint every 5 epochs
  save_best_only: false        # Also save best model
  keep_last_n: 3               # Keep last 3 checkpoints only

system:
  device: "cuda"               # CUDA for GPU training
  seed: 42                     # For reproducibility
  cudnn_benchmark: true        # Faster training on fixed input size
                               # Safe to enable on RTX 5090
  cudnn_deterministic: false   # Set to true for full reproducibility
                               # (slight speed penalty)

# Logging and monitoring
logging:
  log_interval: 10             # Log every 10 batches
  save_interval: 100           # Save checkpoint every 100 batches

  # Metrics to track
  metrics:
    - accuracy
    - f1_score
    - cohen_kappa
    - confusion_matrix

  # TensorBoard
  use_tensorboard: true
  tensorboard_dir: "/results/retfound_lora/tensorboard"

# Evaluation
evaluation:
  eval_interval: 1             # Evaluate every epoch
  test_at_end: true            # Run final test after training

  # Metrics
  compute_per_class_metrics: true
  save_predictions: true
  save_confusion_matrix: true

# Performance expectations on RTX 5090:
# =========================================
# Training time:     ~3-3.5 hours (20 epochs)
# Memory usage:      ~12GB / 32GB VRAM
# Batch size:        64 (2x default)
# Trainable params:  ~1.5M (lora_r=16)
# Total params:      ~303M
# Trainable %:       ~0.5%
#
# Comparison to RTX 3090:
# - 40% faster training
# - 2x larger batch size possible
# - More stable gradients
# - Potentially +0.5-1% accuracy improvement
#
# Cost optimization:
# - Faster training = less rental time
# - Higher performance = better results per dollar
# - Can run multiple experiments if needed (plenty of VRAM)

# Notes on RTX 5090 optimization:
# ================================
# 1. batch_size=64 is optimal for 32GB VRAM
#    - Uses ~12GB for training
#    - Leaves 20GB headroom for safety
#    - More stable gradients than smaller batches
#
# 2. mixed_precision=true is ESSENTIAL
#    - Leverages Tensor Cores
#    - 2x speedup with no accuracy loss
#    - Standard practice for modern GPUs
#
# 3. lora_r=16 for better capacity
#    - More parameters = better performance
#    - Still only 0.5% of total parameters
#    - Memory is not a concern on RTX 5090
#
# 4. num_workers=8 prevents data bottleneck
#    - RTX 5090 is fast, don't let data loading slow it down
#    - 8 workers can saturate even PCIe 4.0
#
# 5. Consider gradient accumulation for even larger effective batch sizes
#    - Can simulate batch_size=128 or 256
#    - Useful for very small datasets
#    - Add gradient_accumulation_steps: 2 if needed
