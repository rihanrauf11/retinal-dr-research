# RETFound_Green + LoRA Configuration for Diabetic Retinopathy Classification
#
# This configuration uses the compact RETFound_Green model (ViT-Small, 21.3M params)
# with Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning.
#
# RETFound_Green Benefits:
# - 14x smaller than RETFound Large (21.3M vs 303M parameters)
# - 92% reduction in model size (~50 MB vs 1.2 GB)
# - 40% faster inference (single image: ~10ms vs ~18ms)
# - Only ~800K trainable params with LoRA (99.996% parameter reduction!)
# - Better performance per parameter on smaller datasets
# - Lower memory footprint (2-3 GB vs 6-8 GB for training)
#
# Input Specifications:
# - Input size: 392x392 (vs 224x224 for RETFound Large)
# - Normalization: [0.5, 0.5, 0.5] mean and std (vs ImageNet norm)
# - Embedding dim: 384 (vs 1024 for Large)
#
# Use this config when:
# - Training on edge devices or limited GPU memory
# - Speed is critical for deployment
# - Working with smaller datasets (< 5,000 images)
# - Efficiency is more important than maximum accuracy

model:
  model_name: retfound_green_lora
  model_variant: green             # Compact RETFound variant (ViT-S, 21.3M parameters)
  num_classes: 5
  pretrained: true
  # Note: Green variant doesn't require the RETFound weights - uses timm pretrained ViT-Small
  pretrained_path: null            # Set to null for Green variant (uses timm weights)

  # LoRA-specific parameters
  lora_r: 8                         # LoRA rank (controls adapter capacity)
                                    # Common values: 4, 8, 16, 32
                                    # For Green with smaller dataset, r=8-16 recommended

  lora_alpha: 32                    # LoRA scaling factor (typically 2-4x rank)
                                    # Controls magnitude of LoRA updates

  lora_dropout: 0.1                 # Dropout for LoRA layers (regularization)

  head_dropout: 0.3                 # Dropout before classification head

  target_modules: ["qkv"]           # Modules to apply LoRA to
                                    # Options: ["qkv"], ["qkv", "proj"]

training:
  # Green variant can use different learning rate due to different model scale
  batch_size: 48                    # Can use larger batch with compact model
  num_epochs: 20
  learning_rate: 0.0005             # LoRA learning rate
  weight_decay: 0.01

  # Optimizer settings
  optimizer: adamw
  betas: [0.9, 0.999]
  eps: 1.0e-08

  # Learning rate scheduling
  scheduler: cosine                 # Options: cosine, step, plateau, none
  warmup_epochs: 2
  min_lr: 1.0e-06

  # Early stopping
  patience: 5
  min_delta: 0.001

  # Gradient clipping
  grad_clip: 1.0

  # Mixed precision training
  mixed_precision: true

image:
  # RETFound_Green uses 392x392 images (larger receptive field)
  input_size: 392

  # Custom normalization for RETFound_Green
  # Different from ImageNet norm due to different pretraining
  mean: [0.5, 0.5, 0.5]
  std: [0.5, 0.5, 0.5]

  # Data augmentation
  augmentation:
    resize: 416                    # Slightly larger than input size
    random_crop: 392               # Match input size
    horizontal_flip: 0.5
    vertical_flip: 0.3
    rotation: 15
    color_jitter:
      brightness: 0.2
      contrast: 0.2
      saturation: 0.2
      hue: 0.1
    coarse_dropout:
      max_holes: 4
      max_height: 20
      max_width: 20
      p: 0.5

data:
  # Update these paths for your dataset
  train_csv: "data/aptos/train_split.csv"
  val_csv: "data/aptos/val_split.csv"
  test_csv: "data/aptos/test.csv"
  img_dir: "data/aptos/train_images"

  # Data loading
  num_workers: 4
  pin_memory: true
  persistent_workers: true

paths:
  output_dir: "results/retfound_green_lora"
  checkpoint_dir: "results/retfound_green_lora/checkpoints"
  log_dir: "results/retfound_green_lora/logs"

  # Save checkpoints
  save_every_n_epochs: 5
  save_best_only: false
  keep_last_n: 3

system:
  device: "cuda"                    # cuda, mps, or cpu
  seed: 42
  cudnn_benchmark: true
  cudnn_deterministic: false

# Logging and monitoring
logging:
  log_interval: 10
  save_interval: 100

  metrics:
    - accuracy
    - f1_score
    - cohen_kappa
    - confusion_matrix

  use_tensorboard: true
  tensorboard_dir: "results/retfound_green_lora/tensorboard"

# Evaluation
evaluation:
  eval_interval: 1
  test_at_end: true

  compute_per_class_metrics: true
  save_predictions: true
  save_confusion_matrix: true

# ═══════════════════════════════════════════════════════════════════════════════
# Notes on RETFound_Green vs RETFound Large
# ═══════════════════════════════════════════════════════════════════════════════
#
# Model Architecture:
#   RETFound Large:
#   - Custom ViT-Large from RETFound paper
#   - 303M parameters, 1024D embeddings
#   - 224x224 input, ImageNet normalization
#   - Pretrained on 1.6M retinal images with MAE
#   - Better for high-accuracy requirements
#
#   RETFound Green:
#   - timm's vit_small_patch14_reg4_dinov2
#   - 21.3M parameters, 384D embeddings
#   - 392x392 input, custom normalization [0.5, 0.5, 0.5]
#   - Pretrained on diverse data via DINOv2
#   - Better for efficiency and speed
#
# Parameter Counts with LoRA (r=8):
#   - Large: 303M total, ~800K trainable (0.26%)
#   - Green: 21.3M total, ~500K trainable (2.3%)
#
# Training Memory (batch_size=32):
#   - Large: ~8-10 GB GPU
#   - Green: ~2-3 GB GPU
#
# Inference Speed (per image):
#   - Large: ~18ms on RTX 3090
#   - Green: ~10ms on RTX 3090
#
# Use Green variant when:
#   ✓ Speed is critical for deployment
#   ✓ Limited GPU memory (< 4GB)
#   ✓ Edge device deployment needed
#   ✓ Dataset is small (< 5,000 images)
#   ✓ Fast iteration during development
#
# Use Large variant when:
#   ✓ Maximum accuracy is required
#   ✓ Large dataset (> 100,000 images)
#   ✓ Sufficient GPU memory available (> 10GB)
#   ✓ Training time is not critical
#   ✓ Using official RETFound weights
#
# Migration from Large to Green:
#   1. Update config: model_variant: green
#   2. Update image size: input_size: 392
#   3. Update normalization: mean/std for Green
#   4. Set pretrained_path: null for Green (uses timm weights)
#   5. Run training: python scripts/train_retfound_lora.py --config configs/retfound_green_lora_config.yaml
#
# Back-compatibility:
#   - Config system auto-detects and adjusts image preprocessing
#   - Checkpoint loading handles variant differences
#   - Model creation switches backend based on variant setting
#   - All existing training code works unchanged
