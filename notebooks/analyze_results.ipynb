{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Analysis for Diabetic Retinopathy Classification\n",
    "\n",
    "This notebook provides comprehensive visualization and statistical analysis of training results and cross-dataset evaluation for DR classification models.\n",
    "\n",
    "**Contents:**\n",
    "1. Training History Analysis (Loss & Accuracy Curves)\n",
    "2. Cross-Dataset Evaluation Results\n",
    "3. Model Comparison (Baseline vs RETFound vs RETFound+LoRA)\n",
    "4. Generalization Gap Analysis\n",
    "5. Sample Predictions Visualization\n",
    "6. Failure Mode Analysis\n",
    "7. Statistical Significance Testing\n",
    "\n",
    "**Author:** Muhamad Rihan Rauf Azkiya\n",
    "**Date:** October 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import wilcoxon, mannwhitneyu\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "# Deep learning (for loading models if needed)\n",
    "import torch\n",
    "\n",
    "# Configure matplotlib for publication-ready figures\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi': 300,\n",
    "    'savefig.dpi': 300,\n",
    "    'figure.figsize': (10, 6),\n",
    "    'font.size': 11,\n",
    "    'font.family': 'serif',\n",
    "    'axes.labelsize': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.titleweight': 'bold',\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.titlesize': 16,\n",
    "    'figure.titleweight': 'bold',\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3,\n",
    "    'lines.linewidth': 2,\n",
    "})\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Color palette for consistent model colors\n",
    "MODEL_COLORS = {\n",
    "    'baseline': '#1f77b4',      # Blue\n",
    "    'retfound': '#ff7f0e',      # Orange\n",
    "    'retfound_lora': '#2ca02c', # Green\n",
    "}\n",
    "\n",
    "# DR class names\n",
    "DR_CLASSES = ['No DR', 'Mild', 'Moderate', 'Severe', 'PDR']\n",
    "\n",
    "print(\"âœ“ All imports successful\")\n",
    "print(f\"âœ“ Working directory: {os.getcwd()}\")\n",
    "print(f\"âœ“ Matplotlib backend: {plt.get_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Paths\n",
    "\n",
    "**Modify these paths to match your experiment results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CONFIGURATION - MODIFY THESE PATHS FOR YOUR EXPERIMENTS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Project root\n",
    "PROJECT_ROOT = Path('/Users/rihanrauf/Documents/00. Research/01-diabetic-retinopathy-classification')\n",
    "\n",
    "# Training history JSON files (output from train_*.py scripts)\n",
    "TRAINING_HISTORIES = {\n",
    "    'baseline': PROJECT_ROOT / 'results/baseline/logs/training_history.json',\n",
    "    'retfound_lora': PROJECT_ROOT / 'results/retfound_lora/logs/training_history.json',\n",
    "}\n",
    "\n",
    "# Cross-dataset evaluation JSON files (output from evaluate_cross_dataset.py)\n",
    "EVALUATION_RESULTS = {\n",
    "    'baseline': PROJECT_ROOT / 'results/evaluation/baseline/evaluation_results.json',\n",
    "    'retfound_lora': PROJECT_ROOT / 'results/evaluation/retfound_lora/evaluation_results.json',\n",
    "}\n",
    "\n",
    "# Prediction CSV files for sample visualization (optional)\n",
    "PREDICTION_FILES = {\n",
    "    'baseline': PROJECT_ROOT / 'results/evaluation/baseline/predictions/APTOS_predictions.csv',\n",
    "    'retfound_lora': PROJECT_ROOT / 'results/evaluation/retfound_lora/predictions/APTOS_predictions.csv',\n",
    "}\n",
    "\n",
    "# Image directory for sample predictions\n",
    "IMAGE_DIR = PROJECT_ROOT / 'data/aptos/images'\n",
    "\n",
    "# Output directory for generated plots\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'results/analysis'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Configuration paths set\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Training History\n",
    "\n",
    "Load training metrics (loss, accuracy, learning rate) from JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_history(filepath: Path) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Load training history from JSON file.\n",
    "    \n",
    "    Expected structure:\n",
    "    {\n",
    "        'train_loss': [...],\n",
    "        'train_acc': [...],\n",
    "        'val_loss': [...],\n",
    "        'val_acc': [...],\n",
    "        'learning_rate': [...],\n",
    "        'epoch_time': [...],\n",
    "        'class_accuracies': [[...], ...]\n",
    "    }\n",
    "    \"\"\"\n",
    "    if not filepath.exists():\n",
    "        print(f\"âš  Warning: File not found: {filepath}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            history = json.load(f)\n",
    "        \n",
    "        # Validate structure\n",
    "        required_keys = ['train_loss', 'train_acc', 'val_loss', 'val_acc']\n",
    "        if not all(key in history for key in required_keys):\n",
    "            print(f\"âš  Warning: Missing required keys in {filepath}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"âœ“ Loaded {filepath.name}: {len(history['train_loss'])} epochs\")\n",
    "        return history\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error loading {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Load all training histories\n",
    "training_data = {}\n",
    "for model_name, filepath in TRAINING_HISTORIES.items():\n",
    "    history = load_training_history(filepath)\n",
    "    if history is not None:\n",
    "        training_data[model_name] = history\n",
    "\n",
    "print(f\"\\nâœ“ Loaded {len(training_data)} training histories\")\n",
    "for name in training_data.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plot Training & Validation Loss Curves\n",
    "\n",
    "Compare loss curves across different models to assess convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(training_data: Dict, save_path: Optional[Path] = None):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss curves for multiple models.\n",
    "    \"\"\"\n",
    "    if not training_data:\n",
    "        print(\"âš  No training data available\")\n",
    "        return\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Training Loss\n",
    "    for model_name, history in training_data.items():\n",
    "        epochs = range(1, len(history['train_loss']) + 1)\n",
    "        color = MODEL_COLORS.get(model_name, None)\n",
    "        ax1.plot(epochs, history['train_loss'], label=model_name.replace('_', ' ').title(),\n",
    "                color=color, linewidth=2, marker='o', markersize=4, alpha=0.8)\n",
    "    \n",
    "    ax1.set_xlabel('Epoch', fontweight='bold')\n",
    "    ax1.set_ylabel('Training Loss', fontweight='bold')\n",
    "    ax1.set_title('Training Loss Curves', fontweight='bold', fontsize=14)\n",
    "    ax1.legend(loc='best', framealpha=0.9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Validation Loss\n",
    "    for model_name, history in training_data.items():\n",
    "        epochs = range(1, len(history['val_loss']) + 1)\n",
    "        color = MODEL_COLORS.get(model_name, None)\n",
    "        ax2.plot(epochs, history['val_loss'], label=model_name.replace('_', ' ').title(),\n",
    "                color=color, linewidth=2, marker='s', markersize=4, alpha=0.8)\n",
    "    \n",
    "    ax2.set_xlabel('Epoch', fontweight='bold')\n",
    "    ax2.set_ylabel('Validation Loss', fontweight='bold')\n",
    "    ax2.set_title('Validation Loss Curves', fontweight='bold', fontsize=14)\n",
    "    ax2.legend(loc='best', framealpha=0.9)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"âœ“ Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Generate plot\n",
    "plot_loss_curves(training_data, OUTPUT_DIR / 'loss_curves.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plot Training & Validation Accuracy Curves\n",
    "\n",
    "Compare accuracy progression and identify best performance epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_curves(training_data: Dict, save_path: Optional[Path] = None):\n",
    "    \"\"\"\n",
    "    Plot training and validation accuracy curves with peak annotations.\n",
    "    \"\"\"\n",
    "    if not training_data:\n",
    "        print(\"âš  No training data available\")\n",
    "        return\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Training Accuracy\n",
    "    for model_name, history in training_data.items():\n",
    "        epochs = range(1, len(history['train_acc']) + 1)\n",
    "        color = MODEL_COLORS.get(model_name, None)\n",
    "        ax1.plot(epochs, history['train_acc'], label=model_name.replace('_', ' ').title(),\n",
    "                color=color, linewidth=2, marker='o', markersize=4, alpha=0.8)\n",
    "    \n",
    "    ax1.set_xlabel('Epoch', fontweight='bold')\n",
    "    ax1.set_ylabel('Training Accuracy (%)', fontweight='bold')\n",
    "    ax1.set_title('Training Accuracy Curves', fontweight='bold', fontsize=14)\n",
    "    ax1.legend(loc='best', framealpha=0.9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim([0, 100])\n",
    "    \n",
    "    # Plot 2: Validation Accuracy with peak annotations\n",
    "    for model_name, history in training_data.items():\n",
    "        epochs = range(1, len(history['val_acc']) + 1)\n",
    "        color = MODEL_COLORS.get(model_name, None)\n",
    "        ax2.plot(epochs, history['val_acc'], label=model_name.replace('_', ' ').title(),\n",
    "                color=color, linewidth=2, marker='s', markersize=4, alpha=0.8)\n",
    "        \n",
    "        # Annotate best accuracy\n",
    "        best_acc = max(history['val_acc'])\n",
    "        best_epoch = history['val_acc'].index(best_acc) + 1\n",
    "        ax2.annotate(f'{best_acc:.1f}%',\n",
    "                    xy=(best_epoch, best_acc),\n",
    "                    xytext=(10, 10), textcoords='offset points',\n",
    "                    ha='left', fontsize=9,\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.3),\n",
    "                    arrowprops=dict(arrowstyle='->', color=color, lw=1.5))\n",
    "    \n",
    "    ax2.set_xlabel('Epoch', fontweight='bold')\n",
    "    ax2.set_ylabel('Validation Accuracy (%)', fontweight='bold')\n",
    "    ax2.set_title('Validation Accuracy Curves', fontweight='bold', fontsize=14)\n",
    "    ax2.legend(loc='best', framealpha=0.9)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim([0, 100])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"âœ“ Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nðŸ“Š Best Validation Accuracies:\")\n",
    "    for model_name, history in training_data.items():\n",
    "        best_acc = max(history['val_acc'])\n",
    "        best_epoch = history['val_acc'].index(best_acc) + 1\n",
    "        print(f\"  {model_name:20s}: {best_acc:5.2f}% at epoch {best_epoch}\")\n",
    "\n",
    "\n",
    "# Generate plot\n",
    "plot_accuracy_curves(training_data, OUTPUT_DIR / 'accuracy_curves.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Cross-Dataset Evaluation Results\n",
    "\n",
    "Load evaluation metrics from multiple test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_evaluation_results(filepath: Path) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Load cross-dataset evaluation results from JSON.\n",
    "    \n",
    "    Expected structure:\n",
    "    {\n",
    "        'model_info': {...},\n",
    "        'datasets': {\n",
    "            'APTOS': {'accuracy': ..., 'precision_macro': ..., ...},\n",
    "            'Messidor': {...},\n",
    "        },\n",
    "        'summary': {\n",
    "            'mean_accuracy': ...,\n",
    "            'generalization_gap': ...,\n",
    "            'best_dataset': ...,\n",
    "            'worst_dataset': ...\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    if not filepath.exists():\n",
    "        print(f\"âš  Warning: File not found: {filepath}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            results = json.load(f)\n",
    "        \n",
    "        # Validate structure\n",
    "        if 'datasets' not in results:\n",
    "            print(f\"âš  Warning: Missing 'datasets' key in {filepath}\")\n",
    "            return None\n",
    "        \n",
    "        num_datasets = len(results['datasets'])\n",
    "        print(f\"âœ“ Loaded {filepath.name}: {num_datasets} datasets\")\n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error loading {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Load all evaluation results\n",
    "evaluation_data = {}\n",
    "for model_name, filepath in EVALUATION_RESULTS.items():\n",
    "    results = load_evaluation_results(filepath)\n",
    "    if results is not None:\n",
    "        evaluation_data[model_name] = results\n",
    "\n",
    "print(f\"\\nâœ“ Loaded {len(evaluation_data)} evaluation results\")\n",
    "for name, results in evaluation_data.items():\n",
    "    datasets = list(results['datasets'].keys())\n",
    "    print(f\"  - {name}: {datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Dataset Accuracy Comparison\n",
    "\n",
    "Compare model performance across different test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cross_dataset_comparison(evaluation_data: Dict, save_path: Optional[Path] = None):\n",
    "    \"\"\"\n",
    "    Create grouped bar chart comparing metrics across datasets.\n",
    "    \"\"\"\n",
    "    if not evaluation_data:\n",
    "        print(\"âš  No evaluation data available\")\n",
    "        return\n",
    "    \n",
    "    # Extract dataset names (use first model's datasets)\n",
    "    first_model = list(evaluation_data.values())[0]\n",
    "    dataset_names = list(first_model['datasets'].keys())\n",
    "    \n",
    "    # Metrics to plot\n",
    "    metrics = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro']\n",
    "    metric_labels = ['Accuracy', 'F1-Score', 'Precision', 'Recall']\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (metric, label) in enumerate(zip(metrics, metric_labels)):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Prepare data\n",
    "        x = np.arange(len(dataset_names))\n",
    "        width = 0.8 / len(evaluation_data)\n",
    "        \n",
    "        # Plot bars for each model\n",
    "        for i, (model_name, results) in enumerate(evaluation_data.items()):\n",
    "            values = [results['datasets'][ds][metric] * 100 for ds in dataset_names]\n",
    "            color = MODEL_COLORS.get(model_name, None)\n",
    "            offset = (i - len(evaluation_data)/2 + 0.5) * width\n",
    "            \n",
    "            bars = ax.bar(x + offset, values, width, label=model_name.replace('_', ' ').title(),\n",
    "                         color=color, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{height:.1f}', ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        ax.set_xlabel('Dataset', fontweight='bold')\n",
    "        ax.set_ylabel(f'{label} (%)', fontweight='bold')\n",
    "        ax.set_title(f'{label} Across Datasets', fontweight='bold', fontsize=12)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(dataset_names, rotation=15, ha='right')\n",
    "        ax.legend(loc='best', framealpha=0.9)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        ax.set_ylim([0, 100])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"âœ“ Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Generate plot\n",
    "plot_cross_dataset_comparison(evaluation_data, OUTPUT_DIR / 'cross_dataset_comparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generalization Gap Analysis\n",
    "\n",
    "Visualize the generalization gap (max - min accuracy) for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generalization_gap(evaluation_data: Dict, save_path: Optional[Path] = None):\n",
    "    \"\"\"\n",
    "    Plot generalization gap with best/worst dataset annotations.\n",
    "    \"\"\"\n",
    "    if not evaluation_data:\n",
    "        print(\"âš  No evaluation data available\")\n",
    "        return\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Generalization Gap Bar Chart\n",
    "    model_names = []\n",
    "    gaps = []\n",
    "    mean_accs = []\n",
    "    \n",
    "    for model_name, results in evaluation_data.items():\n",
    "        model_names.append(model_name.replace('_', ' ').title())\n",
    "        \n",
    "        if 'summary' in results:\n",
    "            gaps.append(results['summary']['generalization_gap'] * 100)\n",
    "            mean_accs.append(results['summary']['mean_accuracy'] * 100)\n",
    "        else:\n",
    "            # Calculate manually if not in summary\n",
    "            accuracies = [ds['accuracy'] for ds in results['datasets'].values()]\n",
    "            gaps.append((max(accuracies) - min(accuracies)) * 100)\n",
    "            mean_accs.append(np.mean(accuracies) * 100)\n",
    "    \n",
    "    colors = [MODEL_COLORS.get(m.lower().replace(' ', '_'), None) for m in model_names]\n",
    "    bars = ax1.bar(model_names, gaps, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Add horizontal line for \"acceptable\" threshold (e.g., 10%)\n",
    "    ax1.axhline(y=10, color='red', linestyle='--', linewidth=2, alpha=0.5, label='Target Threshold (10%)')\n",
    "    \n",
    "    ax1.set_ylabel('Generalization Gap (%)', fontweight='bold')\n",
    "    ax1.set_title('Generalization Gap: Max - Min Accuracy', fontweight='bold', fontsize=14)\n",
    "    ax1.legend(loc='best', framealpha=0.9)\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 2: Mean Accuracy with Error Bars (representing gap)\n",
    "    yerr = [gap/2 for gap in gaps]  # Use half gap as error bar\n",
    "    ax2.bar(model_names, mean_accs, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "    ax2.errorbar(model_names, mean_accs, yerr=yerr, fmt='none', ecolor='black',\n",
    "                 capsize=10, capthick=2, linewidth=2, alpha=0.7)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (name, mean_acc) in enumerate(zip(model_names, mean_accs)):\n",
    "        ax2.text(i, mean_acc + yerr[i] + 1,\n",
    "                f'{mean_acc:.2f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    ax2.set_ylabel('Mean Accuracy (%)', fontweight='bold')\n",
    "    ax2.set_title('Mean Accuracy Across Datasets (Â±Gap/2)', fontweight='bold', fontsize=14)\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    ax2.set_ylim([0, 100])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"âœ“ Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"\\nðŸ“Š Generalization Gap Summary:\")\n",
    "    print(\"â”€\" * 80)\n",
    "    print(f\"{'Model':<25} {'Mean Acc':<12} {'Gap':<10} {'Best Dataset':<20} {'Worst Dataset':<20}\")\n",
    "    print(\"â”€\" * 80)\n",
    "    \n",
    "    for model_name, results in evaluation_data.items():\n",
    "        if 'summary' in results:\n",
    "            mean_acc = results['summary']['mean_accuracy'] * 100\n",
    "            gap = results['summary']['generalization_gap'] * 100\n",
    "            best_ds = results['summary']['best_dataset']\n",
    "            worst_ds = results['summary']['worst_dataset']\n",
    "        else:\n",
    "            accuracies = {ds: data['accuracy'] for ds, data in results['datasets'].items()}\n",
    "            mean_acc = np.mean(list(accuracies.values())) * 100\n",
    "            gap = (max(accuracies.values()) - min(accuracies.values())) * 100\n",
    "            best_ds = max(accuracies, key=accuracies.get)\n",
    "            worst_ds = min(accuracies, key=accuracies.get)\n",
    "        \n",
    "        print(f\"{model_name:<25} {mean_acc:>6.2f}%     {gap:>6.2f}%   {best_ds:<20} {worst_ds:<20}\")\n",
    "    \n",
    "    print(\"â”€\" * 80)\n",
    "\n",
    "\n",
    "# Generate plot\n",
    "plot_generalization_gap(evaluation_data, OUTPUT_DIR / 'generalization_gap.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sample Predictions with Images\n",
    "\n",
    "Visualize correct and incorrect predictions with actual fundus images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_predictions(predictions_csv: Path, image_dir: Path, \n",
    "                           num_correct: int = 5, num_incorrect: int = 5,\n",
    "                           save_path: Optional[Path] = None):\n",
    "    \"\"\"\n",
    "    Plot sample predictions (correct vs incorrect) with images.\n",
    "    \n",
    "    Requires:\n",
    "        - predictions_csv: CSV with columns [image_id, true_label, predicted_label, confidence]\n",
    "        - image_dir: Directory containing images\n",
    "    \"\"\"\n",
    "    if not predictions_csv.exists():\n",
    "        print(f\"âš  Predictions file not found: {predictions_csv}\")\n",
    "        return\n",
    "    \n",
    "    if not image_dir.exists():\n",
    "        print(f\"âš  Image directory not found: {image_dir}\")\n",
    "        return\n",
    "    \n",
    "    # Load predictions\n",
    "    df = pd.read_csv(predictions_csv)\n",
    "    \n",
    "    # Separate correct and incorrect predictions\n",
    "    correct_df = df[df['true_label'] == df['predicted_label']].sample(min(num_correct, len(df)))\n",
    "    incorrect_df = df[df['true_label'] != df['predicted_label']].sample(min(num_incorrect, len(df)))\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(2, max(num_correct, num_incorrect), figsize=(20, 8))\n",
    "    \n",
    "    # Plot correct predictions\n",
    "    for idx, (_, row) in enumerate(correct_df.iterrows()):\n",
    "        if idx >= num_correct:\n",
    "            break\n",
    "        \n",
    "        # Find image file\n",
    "        img_path = None\n",
    "        for ext in ['.png', '.jpg', '.jpeg']:\n",
    "            potential_path = image_dir / f\"{row['image_id']}{ext}\"\n",
    "            if potential_path.exists():\n",
    "                img_path = potential_path\n",
    "                break\n",
    "        \n",
    "        if img_path is None:\n",
    "            continue\n",
    "        \n",
    "        # Load and display image\n",
    "        img = Image.open(img_path)\n",
    "        axes[0, idx].imshow(img)\n",
    "        axes[0, idx].axis('off')\n",
    "        \n",
    "        # Add title with prediction info\n",
    "        true_class = DR_CLASSES[int(row['true_label'])]\n",
    "        conf = row.get('confidence', 1.0) * 100\n",
    "        axes[0, idx].set_title(f\"âœ“ {true_class}\\nConf: {conf:.1f}%\",\n",
    "                              fontsize=10, color='green', fontweight='bold')\n",
    "    \n",
    "    # Plot incorrect predictions\n",
    "    for idx, (_, row) in enumerate(incorrect_df.iterrows()):\n",
    "        if idx >= num_incorrect:\n",
    "            break\n",
    "        \n",
    "        # Find image file\n",
    "        img_path = None\n",
    "        for ext in ['.png', '.jpg', '.jpeg']:\n",
    "            potential_path = image_dir / f\"{row['image_id']}{ext}\"\n",
    "            if potential_path.exists():\n",
    "                img_path = potential_path\n",
    "                break\n",
    "        \n",
    "        if img_path is None:\n",
    "            continue\n",
    "        \n",
    "        # Load and display image\n",
    "        img = Image.open(img_path)\n",
    "        axes[1, idx].imshow(img)\n",
    "        axes[1, idx].axis('off')\n",
    "        \n",
    "        # Add title with prediction info\n",
    "        true_class = DR_CLASSES[int(row['true_label'])]\n",
    "        pred_class = DR_CLASSES[int(row['predicted_label'])]\n",
    "        conf = row.get('confidence', 1.0) * 100\n",
    "        axes[1, idx].set_title(f\"âœ— True: {true_class}\\nPred: {pred_class} ({conf:.1f}%)\",\n",
    "                              fontsize=10, color='red', fontweight='bold')\n",
    "    \n",
    "    # Add row labels\n",
    "    fig.text(0.02, 0.75, 'CORRECT\\nPREDICTIONS', fontsize=14, fontweight='bold',\n",
    "             color='green', rotation=90, va='center', ha='center')\n",
    "    fig.text(0.02, 0.25, 'INCORRECT\\nPREDICTIONS', fontsize=14, fontweight='bold',\n",
    "             color='red', rotation=90, va='center', ha='center')\n",
    "    \n",
    "    plt.tight_layout(rect=[0.03, 0, 1, 1])\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"âœ“ Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Generate plot (for first available model)\n",
    "if PREDICTION_FILES:\n",
    "    first_pred_file = list(PREDICTION_FILES.values())[0]\n",
    "    if first_pred_file.exists():\n",
    "        plot_sample_predictions(first_pred_file, IMAGE_DIR,\n",
    "                               save_path=OUTPUT_DIR / 'sample_predictions.png')\n",
    "    else:\n",
    "        print(\"âš  No prediction files found. Run evaluate_cross_dataset.py with --save_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Failure Mode Analysis by Class\n",
    "\n",
    "Analyze which DR classes are most challenging for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_per_class_performance(evaluation_data: Dict, save_path: Optional[Path] = None):\n",
    "    \"\"\"\n",
    "    Create heatmap showing per-class F1 scores across datasets and models.\n",
    "    \"\"\"\n",
    "    if not evaluation_data:\n",
    "        print(\"âš  No evaluation data available\")\n",
    "        return\n",
    "    \n",
    "    # Extract per-class metrics\n",
    "    # Structure: models x (datasets * classes)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(evaluation_data), figsize=(8 * len(evaluation_data), 6))\n",
    "    if len(evaluation_data) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (model_name, results) in enumerate(evaluation_data.items()):\n",
    "        # Build matrix: datasets x classes\n",
    "        dataset_names = list(results['datasets'].keys())\n",
    "        matrix = []\n",
    "        \n",
    "        for ds_name in dataset_names:\n",
    "            ds_metrics = results['datasets'][ds_name]\n",
    "            if 'per_class_metrics' in ds_metrics:\n",
    "                # Extract F1 scores for each class\n",
    "                f1_scores = [ds_metrics['per_class_metrics'][str(i)]['f1-score'] * 100 \n",
    "                           for i in range(5)]\n",
    "                matrix.append(f1_scores)\n",
    "            elif 'f1_per_class' in ds_metrics:\n",
    "                f1_scores = [score * 100 for score in ds_metrics['f1_per_class']]\n",
    "                matrix.append(f1_scores)\n",
    "        \n",
    "        if not matrix:\n",
    "            print(f\"âš  No per-class metrics found for {model_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Create heatmap\n",
    "        matrix = np.array(matrix)\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        sns.heatmap(matrix, annot=True, fmt='.1f', cmap='RdYlGn', vmin=0, vmax=100,\n",
    "                   xticklabels=DR_CLASSES, yticklabels=dataset_names,\n",
    "                   cbar_kws={'label': 'F1-Score (%)'},\n",
    "                   linewidths=0.5, linecolor='gray', ax=ax)\n",
    "        \n",
    "        ax.set_title(f\"{model_name.replace('_', ' ').title()}\\nPer-Class F1 Scores\",\n",
    "                    fontweight='bold', fontsize=14)\n",
    "        ax.set_xlabel('DR Class', fontweight='bold')\n",
    "        ax.set_ylabel('Dataset', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"âœ“ Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Identify worst-performing classes\n",
    "    print(\"\\nðŸ“Š Class-wise Performance Analysis:\")\n",
    "    print(\"â”€\" * 80)\n",
    "    \n",
    "    for model_name, results in evaluation_data.items():\n",
    "        print(f\"\\n{model_name.upper()}:\")\n",
    "        \n",
    "        # Aggregate across datasets\n",
    "        class_f1_scores = {i: [] for i in range(5)}\n",
    "        \n",
    "        for ds_metrics in results['datasets'].values():\n",
    "            if 'per_class_metrics' in ds_metrics:\n",
    "                for i in range(5):\n",
    "                    class_f1_scores[i].append(ds_metrics['per_class_metrics'][str(i)]['f1-score'])\n",
    "            elif 'f1_per_class' in ds_metrics:\n",
    "                for i, score in enumerate(ds_metrics['f1_per_class']):\n",
    "                    class_f1_scores[i].append(score)\n",
    "        \n",
    "        # Calculate mean F1 per class\n",
    "        for i, scores in class_f1_scores.items():\n",
    "            if scores:\n",
    "                mean_f1 = np.mean(scores) * 100\n",
    "                std_f1 = np.std(scores) * 100\n",
    "                print(f\"  {DR_CLASSES[i]:<12}: {mean_f1:5.2f}% (Â±{std_f1:4.2f}%)\")\n",
    "\n",
    "\n",
    "# Generate plot\n",
    "plot_per_class_performance(evaluation_data, OUTPUT_DIR / 'per_class_performance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Statistical Significance Testing\n",
    "\n",
    "Perform rigorous statistical tests to determine if performance differences are significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_significance_tests(evaluation_data: Dict):\n",
    "    \"\"\"\n",
    "    Perform statistical significance tests between models.\n",
    "    \n",
    "    Tests:\n",
    "    1. McNemar's test (for paired predictions on same test set)\n",
    "    2. Bootstrap confidence intervals (for accuracy differences)\n",
    "    3. Wilcoxon signed-rank test (for cross-dataset comparisons)\n",
    "    \"\"\"\n",
    "    if len(evaluation_data) < 2:\n",
    "        print(\"âš  Need at least 2 models for comparison\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STATISTICAL SIGNIFICANCE TESTING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    model_names = list(evaluation_data.keys())\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Test 1: Wilcoxon Signed-Rank Test for Cross-Dataset Performance\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n1. WILCOXON SIGNED-RANK TEST (Cross-Dataset Accuracy)\")\n",
    "    print(\"   H0: Models have equal performance across datasets\")\n",
    "    print(\"   H1: Models have different performance\")\n",
    "    print(\"â”€\" * 80)\n",
    "    \n",
    "    for i in range(len(model_names)):\n",
    "        for j in range(i+1, len(model_names)):\n",
    "            model1, model2 = model_names[i], model_names[j]\n",
    "            results1 = evaluation_data[model1]\n",
    "            results2 = evaluation_data[model2]\n",
    "            \n",
    "            # Get accuracies across common datasets\n",
    "            common_datasets = set(results1['datasets'].keys()) & set(results2['datasets'].keys())\n",
    "            \n",
    "            if len(common_datasets) < 2:\n",
    "                print(f\"  âš  Not enough common datasets for {model1} vs {model2}\")\n",
    "                continue\n",
    "            \n",
    "            accs1 = [results1['datasets'][ds]['accuracy'] for ds in common_datasets]\n",
    "            accs2 = [results2['datasets'][ds]['accuracy'] for ds in common_datasets]\n",
    "            \n",
    "            # Perform Wilcoxon test\n",
    "            try:\n",
    "                statistic, p_value = wilcoxon(accs1, accs2)\n",
    "                \n",
    "                print(f\"\\n  {model1} vs {model2}:\")\n",
    "                print(f\"    Datasets: {list(common_datasets)}\")\n",
    "                print(f\"    Mean Acc: {np.mean(accs1)*100:.2f}% vs {np.mean(accs2)*100:.2f}%\")\n",
    "                print(f\"    Wilcoxon Statistic: {statistic:.4f}\")\n",
    "                print(f\"    P-value: {p_value:.4f}\")\n",
    "                \n",
    "                if p_value < 0.001:\n",
    "                    print(f\"    Result: *** HIGHLY SIGNIFICANT (p < 0.001) ***\")\n",
    "                elif p_value < 0.01:\n",
    "                    print(f\"    Result: ** SIGNIFICANT (p < 0.01) **\")\n",
    "                elif p_value < 0.05:\n",
    "                    print(f\"    Result: * SIGNIFICANT (p < 0.05) *\")\n",
    "                else:\n",
    "                    print(f\"    Result: NOT SIGNIFICANT (p >= 0.05)\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"  âœ— Test failed for {model1} vs {model2}: {e}\")\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Test 2: Bootstrap Confidence Intervals for Accuracy Difference\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n\\n2. BOOTSTRAP CONFIDENCE INTERVALS (95% CI for Accuracy Difference)\")\n",
    "    print(\"   Number of bootstrap samples: 10,000\")\n",
    "    print(\"â”€\" * 80)\n",
    "    \n",
    "    n_bootstrap = 10000\n",
    "    \n",
    "    for i in range(len(model_names)):\n",
    "        for j in range(i+1, len(model_names)):\n",
    "            model1, model2 = model_names[i], model_names[j]\n",
    "            results1 = evaluation_data[model1]\n",
    "            results2 = evaluation_data[model2]\n",
    "            \n",
    "            # Get predictions for common datasets\n",
    "            common_datasets = set(results1['datasets'].keys()) & set(results2['datasets'].keys())\n",
    "            \n",
    "            if not common_datasets:\n",
    "                continue\n",
    "            \n",
    "            # Aggregate predictions across datasets\n",
    "            all_preds1, all_preds2, all_true = [], [], []\n",
    "            \n",
    "            for ds in common_datasets:\n",
    "                ds1 = results1['datasets'][ds]\n",
    "                ds2 = results2['datasets'][ds]\n",
    "                \n",
    "                if 'predictions' in ds1 and 'ground_truth' in ds1:\n",
    "                    all_preds1.extend(ds1['predictions'])\n",
    "                    all_preds2.extend(ds2['predictions'])\n",
    "                    all_true.extend(ds1['ground_truth'])\n",
    "            \n",
    "            if not all_true:\n",
    "                print(f\"  âš  No predictions available for {model1} vs {model2}\")\n",
    "                continue\n",
    "            \n",
    "            all_preds1 = np.array(all_preds1)\n",
    "            all_preds2 = np.array(all_preds2)\n",
    "            all_true = np.array(all_true)\n",
    "            \n",
    "            # Bootstrap\n",
    "            differences = []\n",
    "            n_samples = len(all_true)\n",
    "            \n",
    "            for _ in range(n_bootstrap):\n",
    "                # Sample with replacement\n",
    "                indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "                \n",
    "                acc1 = np.mean(all_preds1[indices] == all_true[indices])\n",
    "                acc2 = np.mean(all_preds2[indices] == all_true[indices])\n",
    "                \n",
    "                differences.append(acc1 - acc2)\n",
    "            \n",
    "            differences = np.array(differences)\n",
    "            \n",
    "            # Calculate 95% CI\n",
    "            ci_lower = np.percentile(differences, 2.5) * 100\n",
    "            ci_upper = np.percentile(differences, 97.5) * 100\n",
    "            mean_diff = np.mean(differences) * 100\n",
    "            \n",
    "            print(f\"\\n  {model1} vs {model2}:\")\n",
    "            print(f\"    Samples: {n_samples}\")\n",
    "            print(f\"    Mean Accuracy Difference: {mean_diff:+.2f}%\")\n",
    "            print(f\"    95% CI: [{ci_lower:+.2f}%, {ci_upper:+.2f}%]\")\n",
    "            \n",
    "            if ci_lower > 0:\n",
    "                print(f\"    Result: {model1} SIGNIFICANTLY BETTER (CI does not include 0)\")\n",
    "            elif ci_upper < 0:\n",
    "                print(f\"    Result: {model2} SIGNIFICANTLY BETTER (CI does not include 0)\")\n",
    "            else:\n",
    "                print(f\"    Result: NO SIGNIFICANT DIFFERENCE (CI includes 0)\")\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Test 3: McNemar's Test (for paired predictions)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n\\n3. MCNEMAR'S TEST (Paired Predictions)\")\n",
    "    print(\"   H0: Models have equal error rates\")\n",
    "    print(\"   H1: Models have different error rates\")\n",
    "    print(\"â”€\" * 80)\n",
    "    \n",
    "    for i in range(len(model_names)):\n",
    "        for j in range(i+1, len(model_names)):\n",
    "            model1, model2 = model_names[i], model_names[j]\n",
    "            results1 = evaluation_data[model1]\n",
    "            results2 = evaluation_data[model2]\n",
    "            \n",
    "            # Get predictions for common datasets\n",
    "            common_datasets = set(results1['datasets'].keys()) & set(results2['datasets'].keys())\n",
    "            \n",
    "            # Aggregate predictions\n",
    "            all_preds1, all_preds2, all_true = [], [], []\n",
    "            \n",
    "            for ds in common_datasets:\n",
    "                ds1 = results1['datasets'][ds]\n",
    "                ds2 = results2['datasets'][ds]\n",
    "                \n",
    "                if 'predictions' in ds1 and 'ground_truth' in ds1:\n",
    "                    all_preds1.extend(ds1['predictions'])\n",
    "                    all_preds2.extend(ds2['predictions'])\n",
    "                    all_true.extend(ds1['ground_truth'])\n",
    "            \n",
    "            if not all_true:\n",
    "                continue\n",
    "            \n",
    "            all_preds1 = np.array(all_preds1)\n",
    "            all_preds2 = np.array(all_preds2)\n",
    "            all_true = np.array(all_true)\n",
    "            \n",
    "            # Create contingency table\n",
    "            # Rows: Model1 correct/incorrect, Cols: Model2 correct/incorrect\n",
    "            correct1 = (all_preds1 == all_true)\n",
    "            correct2 = (all_preds2 == all_true)\n",
    "            \n",
    "            both_correct = np.sum(correct1 & correct2)\n",
    "            both_incorrect = np.sum(~correct1 & ~correct2)\n",
    "            m1_correct_m2_incorrect = np.sum(correct1 & ~correct2)\n",
    "            m1_incorrect_m2_correct = np.sum(~correct1 & correct2)\n",
    "            \n",
    "            # McNemar's test focuses on discordant pairs\n",
    "            table = [[both_correct, m1_correct_m2_incorrect],\n",
    "                    [m1_incorrect_m2_correct, both_incorrect]]\n",
    "            \n",
    "            try:\n",
    "                result = mcnemar(table, exact=False, correction=True)\n",
    "                \n",
    "                print(f\"\\n  {model1} vs {model2}:\")\n",
    "                print(f\"    Both correct: {both_correct}\")\n",
    "                print(f\"    Both incorrect: {both_incorrect}\")\n",
    "                print(f\"    {model1} correct, {model2} incorrect: {m1_correct_m2_incorrect}\")\n",
    "                print(f\"    {model1} incorrect, {model2} correct: {m1_incorrect_m2_correct}\")\n",
    "                print(f\"    McNemar Statistic: {result.statistic:.4f}\")\n",
    "                print(f\"    P-value: {result.pvalue:.4f}\")\n",
    "                \n",
    "                if result.pvalue < 0.001:\n",
    "                    print(f\"    Result: *** HIGHLY SIGNIFICANT (p < 0.001) ***\")\n",
    "                elif result.pvalue < 0.01:\n",
    "                    print(f\"    Result: ** SIGNIFICANT (p < 0.01) **\")\n",
    "                elif result.pvalue < 0.05:\n",
    "                    print(f\"    Result: * SIGNIFICANT (p < 0.05) *\")\n",
    "                else:\n",
    "                    print(f\"    Result: NOT SIGNIFICANT (p >= 0.05)\")\n",
    "                \n",
    "                # Interpret which model is better\n",
    "                if result.pvalue < 0.05:\n",
    "                    if m1_correct_m2_incorrect > m1_incorrect_m2_correct:\n",
    "                        print(f\"    Interpretation: {model1} performs significantly BETTER\")\n",
    "                    else:\n",
    "                        print(f\"    Interpretation: {model2} performs significantly BETTER\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"  âœ— Test failed: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"LEGEND:\")\n",
    "    print(\"  ***: p < 0.001 (highly significant)\")\n",
    "    print(\"  **:  p < 0.01  (significant)\")\n",
    "    print(\"  *:   p < 0.05  (significant)\")\n",
    "    print(\"       p >= 0.05 (not significant)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "\n",
    "# Perform statistical tests\n",
    "statistical_significance_tests(evaluation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides comprehensive analysis of:\n",
    "- **Training dynamics**: Loss/accuracy curves show convergence behavior\n",
    "- **Cross-dataset performance**: Comparison across multiple test sets\n",
    "- **Generalization ability**: Gap between best and worst performance\n",
    "- **Qualitative analysis**: Sample predictions with images\n",
    "- **Failure modes**: Per-class performance breakdown\n",
    "- **Statistical rigor**: Significance testing with multiple methods\n",
    "\n",
    "All visualizations are publication-ready (300 DPI, proper styling)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
